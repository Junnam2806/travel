# -*- coding: utf-8 -*-
"""221IS9902_Group7_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HbTm_NikD4Z2p-DlgnuQwuCju1tgm_dB

# INSTALL LIBRARY
"""

!pip install requests
!pip install pd
!pip install wordcloud

!pip install vaderSentiment

!pip install googletrans==4.0.0-rc1

! pip install -U textblob
! python -m textblob.download_corpora

"""# IMPORT LIBRARY

Import usually use library
"""

# Commented out IPython magic to ensure Python compatibility.
from numpy import rec
import requests
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt             #visualisation
# %matplotlib inline
sns.set(color_codes=True)
plt.style.use('fivethirtyeight')
from sklearn.model_selection import train_test_split

"""Import odd library"""

from googletrans import Translator
import time
import traceback

"""Add library to download files"""

from google.colab import files

"""Translate"""

from googletrans import Translator
translator = Translator()

import nltk
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer = WordNetLemmatizer()

import vaderSentiment

from nltk.corpus import stopwords

from collections import Counter

"""Style wordcloud

Import library for TF-IDF
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

from itertools import islice

"""Textblob"""

from textblob import TextBlob

"""# TRAVELOKA REVIEWS

###Crawling data
"""

cookies = {
    '_gcl_au': '1.1.392015061.1664942594',
    'g_state': '{"i_l":0}',
    '_fbp': 'fb.1.1664942642116.1608722463',
    'G_ENABLED_IDPS': 'google',
    '_gac_UA-29776811-12': '1.1665471561.Cj0KCQjwhY-aBhCUARIsALNIC044jf7vH3ak5X5f1fIcAPa0bOcqyAEuU2T87_X63FklWPt5T-FESZQaAvofEALw_wcB',
    'tvl': 'qgdHX7GvehrD9XH5a3S4PdE8AYpuF3hYPaT5bxhY7Zb1rMY5TK425kIv5ETJRXLCHeSRMonmaMv0od7JDeBrc6DBkhayi5gQ9Dpvi2lz6w2oLTfDdPL1xT79VU0AZGQMM9o9QzWqp1ZS/a118SM5D+1Jxf+oecGs1Um1EPNjo8VURN3ThgD8Mf5lG13Ci4j5hI7fqyCyEmdQUhtDwGw8cVjAoQJMjZTOsRzZHor+vWeay3MPPOB9sZ/UZ/Krc5UECFnqYYpMSCRL8yPQAXFPMQ==',
    '_gid': 'GA1.2.379556567.1666197141',
    'tv-repeat-visit': 'true',
    'selectedCurrency': 'EUR',
    'selectedLocale': 'en_EN',
    '_ga_RSRSMMBH0X': 'GS1.1.1666233648.18.1.1666233694.14.0.0',
    'tvs': 'qgdHX7GvehrD9XH5a3S4PXWKx93/3Xi103f/kPpnhg1IQez7AjqOPow88qqCMiL7CqvJjpn5Z2svD8QZzAmUN8I7bk86ki9gpnRvB7n1SMg3zgM8GEQUzXoABab35F/lt0q47TyT+ecCF4TDzdkuTYR+PTB5vVtlAZSjbXhlEL9MVeoaxEQheXCzmm0cEa6sfX20ZzuJzpCR3ljpziEyvAE6sRwDYjvvt4ACkV8PmtfgQe/dpDKe1splNHwbCZYHHKMAHEOIS2CR6oHc11VkcMOrX6BQKO3p75zJ993N5ctP5lUUeZvqwg8Za/5sXO3LZP+wb33mpHnKyxjbfd5ZlTN3hMzImFcyzvIC2wefxfo=',
    'amp_1a5adb': 'W28TgfofcjM3MKtAsufi5f.MjU0NjAwMDIw..1gfpk7gvl.1gfpk8vhl.7o.0.7o',
    'cto_bundle': 'XKyIrl9XcGVTRjV3eWttJTJCbWQzRzQlMkZobSUyRklvTWFzTUJ2cWVXaTJrVGE2ZG5SRzdHWmpLR05BOTEzRGV4bGxpTnZLU0x4TWxaS3I2cUNpUHVCdWkySFJnUGRuSSUyQmdNajREbmhnWlZIZ05XQ2t1ZnEwb0pNYnVkJTJGUiUyQmtvMjlIJTJCWGM5RVRGalhKZlRjbWR5czlxSW9WeTZoY1BpUSUzRCUzRA',
    '_ga': 'GA1.2.677393933.1664942594',
}
dfHotel = pd.read_csv("DFHotel.csv")
dfHotel.info()
list_id = []
for record in dfHotel['referer']:
    hotelId = record.split('-')[-1]
    referer = record



    headers = {
            'authority': 'www.traveloka.com',
            'accept': '*/*',
            'accept-language': 'vi-VN,vi;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5',
            # Already added when you pass json=
            # 'content-type': 'application/json',
            # Requests sorts cookies= alphabetically
            # 'cookie': '_gcl_au=1.1.392015061.1664942594; g_state={"i_l":0}; _fbp=fb.1.1664942642116.1608722463; G_ENABLED_IDPS=google; _gac_UA-29776811-12=1.1665471561.Cj0KCQjwhY-aBhCUARIsALNIC044jf7vH3ak5X5f1fIcAPa0bOcqyAEuU2T87_X63FklWPt5T-FESZQaAvofEALw_wcB; tvl=qgdHX7GvehrD9XH5a3S4PdE8AYpuF3hYPaT5bxhY7Zb1rMY5TK425kIv5ETJRXLCHeSRMonmaMv0od7JDeBrc6DBkhayi5gQ9Dpvi2lz6w2oLTfDdPL1xT79VU0AZGQMM9o9QzWqp1ZS/a118SM5D+1Jxf+oecGs1Um1EPNjo8VURN3ThgD8Mf5lG13Ci4j5hI7fqyCyEmdQUhtDwGw8cVjAoQJMjZTOsRzZHor+vWeay3MPPOB9sZ/UZ/Krc5UECFnqYYpMSCRL8yPQAXFPMQ==; _gid=GA1.2.379556567.1666197141; tv-repeat-visit=true; selectedCurrency=EUR; selectedLocale=en_EN; _ga_RSRSMMBH0X=GS1.1.1666233648.18.1.1666233694.14.0.0; tvs=qgdHX7GvehrD9XH5a3S4PXWKx93/3Xi103f/kPpnhg1IQez7AjqOPow88qqCMiL7CqvJjpn5Z2svD8QZzAmUN8I7bk86ki9gpnRvB7n1SMg3zgM8GEQUzXoABab35F/lt0q47TyT+ecCF4TDzdkuTYR+PTB5vVtlAZSjbXhlEL9MVeoaxEQheXCzmm0cEa6sfX20ZzuJzpCR3ljpziEyvAE6sRwDYjvvt4ACkV8PmtfgQe/dpDKe1splNHwbCZYHHKMAHEOIS2CR6oHc11VkcMOrX6BQKO3p75zJ993N5ctP5lUUeZvqwg8Za/5sXO3LZP+wb33mpHnKyxjbfd5ZlTN3hMzImFcyzvIC2wefxfo=; amp_1a5adb=W28TgfofcjM3MKtAsufi5f.MjU0NjAwMDIw..1gfpk7gvl.1gfpk8vhl.7o.0.7o; cto_bundle=XKyIrl9XcGVTRjV3eWttJTJCbWQzRzQlMkZobSUyRklvTWFzTUJ2cWVXaTJrVGE2ZG5SRzdHWmpLR05BOTEzRGV4bGxpTnZLU0x4TWxaS3I2cUNpUHVCdWkySFJnUGRuSSUyQmdNajREbmhnWlZIZ05XQ2t1ZnEwb0pNYnVkJTJGUiUyQmtvMjlIJTJCWGM5RVRGalhKZlRjbWR5czlxSW9WeTZoY1BpUSUzRCUzRA; _ga=GA1.2.677393933.1664942594',
            'origin': 'https://www.traveloka.com',
            'referer': referer,
            'sec-ch-ua': '"Chromium";v="106", "Google Chrome";v="106", "Not;A=Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',
            'x-domain': 'accomContent',
            'x-route-prefix': 'en-en',
        }

    skip = 0
    top = 50
    count = 0

    while True:
            json_data = {
                'fields': [],
                'data': {
                    'filterSortSpec': {
                        'travelTheme': None,
                        'travelType': None,
                        'sortType': 'LANGUAGE',
                        'tagIds': [],
                    },
                    'ascending': True,
                    'reviewLanguage': 'ENGLISH',
                    'hotelId': hotelId,
                    'skip': skip,
                    'top': top,
                },
                'clientInterface': 'desktop',
            }
            try:
                response = requests.post('https://www.traveloka.com/api/v2/hotel/getHotelReviews', cookies=cookies, headers=headers, json=json_data)
                json_travel = response.json()
                collection = []
                review_list = json_travel['data']['reviewList']
                print(len(review_list))
                count += len(review_list)
                if (len(review_list) == 0):
                    break
                for review in review_list:
                    try:
                        reviewTen = review['reviewerName']
                        reviewContent = review['reviewText']
                        score = review['overallScore']
                        referer = review['sharedUrl']

                        danhgia = {
                            'reviewTen' : reviewTen,
                            'reviewContent' : reviewContent,
                            'score' : score,
                            'referer' : referer
                        }

                        collection.append(danhgia)
                        df = pd.DataFrame(collection)
                    except Exception as e:
                          print('')
                skip += top
                print(df)


            except:
                    pass

dfHotel = pd.read_csv("Data_review.csv")
dfHotel.info()

dfHotel

"""Choose specific column for Data"""

df_review = dfHotel[['reviewContent']]
df_review

"""### EDA

Drop unnamed column
"""

from google.colab import drive
drive.mount('/content/drive')

dfHotel = pd.read_csv("Data_review.csv")
dfHotel.head(5)
# To display the top 5 rows

dfHotel.tail(5)
# To display the botton 5 rows

"""Checking the types of data"""

dfHotel.dtypes

"""Dropping irrelevant columns"""

dfHotel = dfHotel.drop(['referer'], axis=1)

"""Renaming the columns"""

dfHotel = dfHotel.rename(columns={"reviewTen": "Name", "reviewContent": "Review", "score": "Score" })
dfHotel.head(5)

"""Dropping the duplicate rows"""

dfHotel.shape

duplicate_rows_df = dfHotel[dfHotel.duplicated()]
print("number of duplicate rows: ", duplicate_rows_df.shape)

"""remove the duplicate data"""

dfHotel.count()
# Used to count the number of rows

dfHotel = dfHotel.drop_duplicates()
dfHotel.head(5)

dfHotel.count()

"""Dropping the missing or null values."""

print(dfHotel.isnull().sum())

dfHotel = dfHotel.dropna()    # Dropping the missing values.
dfHotel.count()

print(dfHotel.isnull().sum())

"""Detecting Outliers"""

sns.boxplot(x=dfHotel['Score'])

dfHotel.info()

"""Customize df"""

df_db = dfHotel['Review']

"""Word display"""

#1. Total No of Char
df_db['num_char'] = df_db.apply(len)
#2. Total No of Words
df_db['num_words'] = df_db.apply(lambda x: len(str(x).split()))
#3. avg word length
df_db['avg_words_length'] = df_db['num_char']/df_db['num_words']

df_db

df_db.describe()

sns.set(rc={"figure.figsize":(20, 10)})
sns.boxplot(data=df_db[["num_char", "num_words","avg_words_length"]], orient="v", width=0.5)

# Construct the scatter plot
p = sns.scatterplot(data=df_db, x="num_char", y="num_words",palette="deep")

"""#### Outliner"""

# IQR
Q1 = np.percentile(df_db['num_char'], 25, interpolation = 'midpoint')
Q3 = np.percentile(df_db['num_char'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
print("Inter Quartile Range:",IQR)

# IQR
Q1 = np.percentile(df_db['num_words'], 25, interpolation = 'midpoint')
Q3 = np.percentile(df_db['num_words'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
print("Inter Quartile Range:",IQR)
print("Upper fence:", Q3+1.5*IQR)

# IQR
Q1 = np.percentile(df_db['avg_words_length'], 25, interpolation = 'midpoint')
Q3 = np.percentile(df_db['avg_words_length'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
print("Inter Quartile Range:",IQR)
print("Upper fence:", Q3+1.5*IQR)
print("Lower fence:", Q1-1.5*IQR)

"""#### Display"""

df_db.loc[df_db.num_char > 304.0]

df_db.loc[df_db.num_words > 49.5]

df_db.loc[df_db.avg_words_length > 10.305934195064632]

df_db.loc[df_db.avg_words_length <= 2.7764394829612207]

df_db = df_db.loc[df_db.avg_words_length <= 10.305934195064632]

"""## Pre-Processing

### Cleaning

Check and Remove all duplicated reviews
"""

df_review.duplicated().sum()

df_review.drop_duplicates(inplace=True)

df = df_review
df

"""#### Creating a new function to Delete special symbol/ word"""

#Remove punctuation
df['reviewContent'] = df['reviewContent'].apply(lambda x: re.sub('[,\.!?]', '', str(x)))

df['reviewContent'] = df['reviewContent'].apply(lambda x : str(x.lower()))

"""Remove stopword"""

stop = stopwords.words('english')

"""### Customize Cleaning

####Custome library
"""

apostrophe_dict = {
"ain't": "am not / are not",
"aren't": "are not / am not",
"can't": "cannot",
"can't've": "cannot have",
"cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he had / he would",
"he'd've": "he would have",
"he'll": "he shall / he will",
"he'll've": "he shall have / he will have",
"he's": "he has / he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how has / how is",
"i'd": "I had / I would",
"i'd've": "I would have",
"i'll": "I shall / I will",
"i'll've": "I shall have / I will have",
"i'm": "I am",
"i've": "I have",
"isn't": "is not",
"it'd": "it had / it would",
"it'd've": "it would have",
"it'll": "it shall / it will",
"it'll've": "it shall have / it will have",
"it's": "it has / it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she had / she would",
"she'd've": "she would have",
"she'll": "she shall / she will",
"she'll've": "she shall have / she will have",
"she's": "she has / she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so as / so is",
"that'd": "that would / that had",
"that'd've": "that would have",
"that's": "that has / that is",
"there'd": "there had / there would",
"there'd've": "there would have",
"there's": "there has / there is",
"they'd": "they had / they would",
"they'd've": "they would have",
"they'll": "they shall / they will",
"they'll've": "they shall have / they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"we'd": "we had / we would",
"we'd've": "we would have",
"we'll": "we will",
"we'll've": "we will have",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what shall / what will",
"what'll've": "what shall have / what will have",
"what're": "what are",
"what's": "what has / what is",
"what've": "what have",
"when's": "when has / when is",
"when've": "when have",
"where'd": "where did",
"where's": "where has / where is",
"where've": "where have",
"who'll": "who shall / who will",
"who'll've": "who shall have / who will have",
"who's": "who has / who is",
"who've": "who have",
"why's": "why has / why is",
"why've": "why have",
"will've": "will have",
"won't": "will not",
"won't've": "will not have",
"would've": "would have",
"wouldn't": "would not",
"wouldn't've": "would not have",
"y'all": "you all",
"y'all'd": "you all would",
"y'all'd've": "you all would have",
"y'all're": "you all are",
"y'all've": "you all have",
"you'd": "you had / you would",
"you'd've": "you would have",
"you'll": "you shall / you will",
"you'll've": "you shall have / you will have",
"you're": "you are",
"you've": "you have",
"apple’s":"apple is",
"dxomark's":"dxomark is"
}
apostrophe_dict

short_word_dict = {
"121": "one to one",
"1/2":"half",
"adn": "any day now",
"afaik": "as far as I know",
"afk": "away from keyboard",
"aight": "alright",
"alol": "actually laughing out loud",
"b4": "before",
"b4n": "bye for now",
"bak": "back at the keyboard",
"bta": "but then again",
"btw": "by the way",
"cid": "crying in disgrace",
"cnp": "continued in my next post",
"cyo": "see you online",
"dbau": "doing business as usual",
"fud": "fear, uncertainty, and doubt",
"fwiw": "for what it's worth",
"fyi": "for your information",
"g": "grin",
"g2g": "got to go",
"ga": "go ahead",
"gal": "get a life",
"gf": "girlfriend",
"gfn": "gone for now",
"gmbo": "giggling my butt off",
"gmta": "great minds think alike",
"h8": "hate",
"hagn": "have a good night",
"hdop": "help delete online predators",
"iac": "in any case",
"ianal": "I am not a lawyer",
"ic": "I see",
"idk": "I don't know",
"imao": "in my arrogant opinion",
"imnsho": "in my not so humble opinion",
"imo": "in my opinion",
"iow": "in other words",
"ipn": "I’m posting naked",
"irl": "in real life",
"jk": "just kidding",
"l8r": "later",
"ldr": "long distance relationship",
"llta": "lots and lots of thunderous applause",
"lmao": "laugh my ass off",
"lol": "laugh out loud",
"ltr": "longterm relationship",
"luv": "love",
"m/f": "male or female",
"m8": "mate",
"milf": "mother I would like to fuck",
"oll": "online love",
"omg": "oh my god",
"otoh": "on the other hand",
"pir": "parent in room",
"ppl": "people",
"r": "are",
"rpg": "role playing games",
"ru": "are you",
"shid": "slaps head in disgust",
"somy": "sick of me yet",
"sot": "short of time",
"thanx": "thanks",
"thx": "thanks",
"u": "you",
"ur": "you are",
"uw": "you’re welcome",
"wb": "welcome back",
"wfm": "works for me",
"wibni": "wouldn't it be nice if",
"wtf": "what the fuck",
"wtg": "way to go",
"wtgp": "want to go private",
"ym": "young man",
"gr8": "great",
"etc":"et cetera"
}

icon_lib = {
    "🤣":"laugh",
    "💔":"hate",
    "😌":"pleased",
    "😂":"funny",
    "🤔":"think",
    "✅":"yes",
    "🗿":"speechless",
    "🙏🏿":"please",
    "🤩":"delighted",
    "🔥":"hot",
    "😭":"cry",
    "⚡️":"fast",
    "❤️":"love",
    "😊":"happy",
    "🥰":"thanks",
    "😢":"unfortunate",
    "☹️":"sad",
    "❌":"no",
    "👍🏻":"okay",
    "💥":"hot",
    "💸":"expensive",
    "💯":"nice",
    "😒":"disappointed",
    "🤯":"surprised",
    "🤲🏿":"want",
    "😂":"hilarious",
    "🥳":"celebrate",
    "🙂":"normal"
}

"""#### Proceed"""

def lookup_dict(text, dictionary):
    for word in text.split():
        if word.lower() in dictionary:
            if word.lower() in text.split():
                text = text.replace(word, dictionary[word.lower()])
    return text

df['reviewContent'] = df['reviewContent'].apply(lambda x: lookup_dict(x,apostrophe_dict))
df

df['reviewContent'] = df['reviewContent'].apply(lambda x: lookup_dict(x,short_word_dict))
df

df['reviewContent'] = df['reviewContent'].apply(lambda x: lookup_dict(x,icon_lib))
df

"""### Stemming & Lemmatizer"""

#POS tagging to exclude word like has/was...
df['lemmatizer'] = df['reviewContent'].apply(lambda x : ' '.join(lemmatizer.lemmatize(word, pos="v") for word in x.split()))

df['comment_word_token'] = df['lemmatizer'].apply(lambda x: ' '.join(word for word in word_tokenize(x) if word not in (stop)))

print(df['comment_word_token'].to_numpy())

df

"""#### Divide into train and test database"""

df_new = df[['reviewContent','comment_word_token']]

df_train, df_test = train_test_split(df_new, test_size=0.2, random_state=0)

df = df_train.reset_index(drop=True)

"""### Rating / Score

#### Polarity Rating
"""

polarity_score = []

for text in df['reviewContent']:
  blob = TextBlob(text)
  for sentence in blob.sentences:
    if sentence.sentiment.polarity > 0.0:
      polarity_score.append('positive')
    elif sentence.sentiment.polarity < 0.0:
      polarity_score.append('negative')
    else:
      polarity_score.append('neutral')

df['score'] = pd.DataFrame(polarity_score)

df

df = pd.read_csv("Update_Data.csv", encoding='latin-1')

df

ax = sns.countplot(df['score'])
plt.sca(ax)
plt.xticks(rotation=90);

plt.savefig("Score.png")

df['rating'] = df['score']
df.rating = df.rating.str.replace('positive', '1')
df.rating = df.rating.str.replace('neutral', '0')
df.rating = df.rating.str.replace('negative', '-1')

temp = df.groupby('score').count()['reviewContent'].reset_index().sort_values(by='reviewContent',ascending=False)
temp.style.background_gradient(cmap='Purples')

from plotly import graph_objs as go
fig = go.Figure(go.Funnelarea(
    text =temp.score,
    values = temp.reviewContent,
    title = {"position": "top center", "text": "Funnel-Chart of ."}
    ))
fig.show()

"""#### VADER rating"""

# function to calculate vader sentiment
def vadersentimentanalysis(review):
    vs = analyzer.polarity_scores(review)
    return vs['compound']
    df['Vader data'] = df['text'].apply(vadersentimentanalysis)

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
def vadar_sentiment(text):
    return analyzer.polarity_scores(text)['compound']

# create new column for vadar compound sentiment score
df['vadar compound'] = df['reviewContent'].apply(vadar_sentiment)

def categorise_sentiment(sentiment, neg_threshold=-0.05, pos_threshold=0.05):
    if sentiment < neg_threshold:
        label = 'negative'
    elif sentiment > pos_threshold:
        label = 'positive'
    else:
        label = 'neutral'
    return label

# new col with vadar sentiment label based on vadar compound score
df['vadar score'] = df['vadar compound'].apply(categorise_sentiment)

df

df['score']

vader_counts = df['vadar score'].value_counts()
tb_counts = df['score'].value_counts()

plt.figure(figsize=(15,7))
plt.subplot(1,2,1)
plt.title("TextBlob results")
plt.pie(tb_counts.values, labels = tb_counts.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)
plt.subplot(1,2,2)
plt.title("VADER results")
plt.pie(vader_counts.values, labels = vader_counts.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)

"""### WordCloud"""

all_words = ' '.join([text for text in df['comment_word_token']])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common words around hotels in Binh Thuan release")
plt.show()

negative_words =' '.join([text for text in df['comment_word_token'][df['score'] == 'negative']])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common negative words around hotels in Binh Thuan release")
plt.show()

positive_words =' '.join([text for text in df['comment_word_token'][df['score'] == 'positive']])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common positive words around hotels in Binh Thuan release")
plt.show()

neutral_words =' '.join([text for text in df['comment_word_token'][df['score'] == 'neutral']])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neutral_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common neutral words around hotels in Binh Thuan release")
plt.show()

"""## Machine learning

### Bag of words
"""

# Chuyển văn bản thành vectơ trên cơ sở tần suất của mỗi từ xuất hiện trong toàn bộ văn bản.
# Xóa các stopwords xuất hiện thường xuyên trong hơn 50% của tài liệu
# Xóa các stopwords xuất hiện không thường xuyên trong ít hơn 2 tài liệu
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english',ngram_range=(1,2))
bow_vectorizer

# duyệt qua comment_word_token và trả về ma trận từ ngữ
bow = bow_vectorizer.fit_transform(df['comment_word_token'])
bow

# Tính tất cả tần suất xuất hiện của n kí tự (n-grams) liên tiếp được tìm thấy trong tất cả dữ liệu
list(islice(bow_vectorizer.vocabulary_.items(), 20))

"""### TF - IDF"""

vectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,2))

train_data_features = vectorizer.fit_transform(df['comment_word_token'])
print(train_data_features.shape)

train,test = train_test_split(df, test_size=0.2, shuffle=True) # chia tệp dữ liệu df ra làm 2 phần, kiểm thử 20% và huấn luyện 80% và có chia ngẫu nhiên
vectorizer = TfidfVectorizer() # tạo ma trận
# đưa những từ có trong cột comment_word_token vào ma trận
vectorizer.fit_transform(train.comment_word_token)
vectorizer.fit_transform(test.comment_word_token)
X_train = vectorizer.transform(train.comment_word_token) # tạo ma trận để huấn luyện mô hình
Y_train = train.score
X_test = vectorizer.transform(test.comment_word_token) # tạo ma trận để kiểm thử mô hình
Y_test = test.score

temp = pd.DataFrame(X_train.todense(),columns = vectorizer.get_feature_names_out())
temp

"""### Random Forest"""

df

"""###SVM"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
classifier_linear = SVC(kernel='linear')
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
classifier_linear = SVC(kernel='sigmoid')
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

"""

```
# Định dạng của đoạn này là mã
```

### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

classifier_linear = LogisticRegression()
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(classification_report(Y_test, pred))

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

classifier_linear = DecisionTreeClassifier()
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

"""

```
# This is formatted as code
```

## Applying ML to test database"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
classifier_linear = SVC(kernel='rbf')
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

test['pred_score'] = pred
test