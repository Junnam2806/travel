# -*- coding: utf-8 -*-
"""221IS9902_Group7_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HbTm_NikD4Z2p-DlgnuQwuCju1tgm_dB

# INSTALL LIBRARY
"""

!pip install requests
!pip install pd
!pip install wordcloud

!pip install vaderSentiment

!pip install googletrans==4.0.0-rc1

! pip install -U textblob
! python -m textblob.download_corpora

"""# IMPORT LIBRARY

Import usually use library
"""

# Commented out IPython magic to ensure Python compatibility.
from numpy import rec
import requests
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt             #visualisation
# %matplotlib inline
sns.set(color_codes=True)
plt.style.use('fivethirtyeight')
from sklearn.model_selection import train_test_split

"""Import odd library"""

from googletrans import Translator
import time
import traceback

"""Add library to download files"""

from google.colab import files

"""Translate"""

from googletrans import Translator
translator = Translator()

import nltk
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer = WordNetLemmatizer()

import vaderSentiment

from nltk.corpus import stopwords

from collections import Counter

"""Style wordcloud

Import library for TF-IDF
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

from itertools import islice

"""Textblob"""

from textblob import TextBlob

"""# TRAVELOKA REVIEWS

###Crawling data
"""

cookies = {
    '_gcl_au': '1.1.392015061.1664942594',
    'g_state': '{"i_l":0}',
    '_fbp': 'fb.1.1664942642116.1608722463',
    'G_ENABLED_IDPS': 'google',
    '_gac_UA-29776811-12': '1.1665471561.Cj0KCQjwhY-aBhCUARIsALNIC044jf7vH3ak5X5f1fIcAPa0bOcqyAEuU2T87_X63FklWPt5T-FESZQaAvofEALw_wcB',
    'tvl': 'qgdHX7GvehrD9XH5a3S4PdE8AYpuF3hYPaT5bxhY7Zb1rMY5TK425kIv5ETJRXLCHeSRMonmaMv0od7JDeBrc6DBkhayi5gQ9Dpvi2lz6w2oLTfDdPL1xT79VU0AZGQMM9o9QzWqp1ZS/a118SM5D+1Jxf+oecGs1Um1EPNjo8VURN3ThgD8Mf5lG13Ci4j5hI7fqyCyEmdQUhtDwGw8cVjAoQJMjZTOsRzZHor+vWeay3MPPOB9sZ/UZ/Krc5UECFnqYYpMSCRL8yPQAXFPMQ==',
    '_gid': 'GA1.2.379556567.1666197141',
    'tv-repeat-visit': 'true',
    'selectedCurrency': 'EUR',
    'selectedLocale': 'en_EN',
    '_ga_RSRSMMBH0X': 'GS1.1.1666233648.18.1.1666233694.14.0.0',
    'tvs': 'qgdHX7GvehrD9XH5a3S4PXWKx93/3Xi103f/kPpnhg1IQez7AjqOPow88qqCMiL7CqvJjpn5Z2svD8QZzAmUN8I7bk86ki9gpnRvB7n1SMg3zgM8GEQUzXoABab35F/lt0q47TyT+ecCF4TDzdkuTYR+PTB5vVtlAZSjbXhlEL9MVeoaxEQheXCzmm0cEa6sfX20ZzuJzpCR3ljpziEyvAE6sRwDYjvvt4ACkV8PmtfgQe/dpDKe1splNHwbCZYHHKMAHEOIS2CR6oHc11VkcMOrX6BQKO3p75zJ993N5ctP5lUUeZvqwg8Za/5sXO3LZP+wb33mpHnKyxjbfd5ZlTN3hMzImFcyzvIC2wefxfo=',
    'amp_1a5adb': 'W28TgfofcjM3MKtAsufi5f.MjU0NjAwMDIw..1gfpk7gvl.1gfpk8vhl.7o.0.7o',
    'cto_bundle': 'XKyIrl9XcGVTRjV3eWttJTJCbWQzRzQlMkZobSUyRklvTWFzTUJ2cWVXaTJrVGE2ZG5SRzdHWmpLR05BOTEzRGV4bGxpTnZLU0x4TWxaS3I2cUNpUHVCdWkySFJnUGRuSSUyQmdNajREbmhnWlZIZ05XQ2t1ZnEwb0pNYnVkJTJGUiUyQmtvMjlIJTJCWGM5RVRGalhKZlRjbWR5czlxSW9WeTZoY1BpUSUzRCUzRA',
    '_ga': 'GA1.2.677393933.1664942594',
}
dfHotel = pd.read_csv("DFHotel.csv")
dfHotel.info()
list_id = []
for record in dfHotel['referer']:
    hotelId = record.split('-')[-1]
    referer = record



    headers = {
            'authority': 'www.traveloka.com',
            'accept': '*/*',
            'accept-language': 'vi-VN,vi;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5',
            # Already added when you pass json=
            # 'content-type': 'application/json',
            # Requests sorts cookies= alphabetically
            # 'cookie': '_gcl_au=1.1.392015061.1664942594; g_state={"i_l":0}; _fbp=fb.1.1664942642116.1608722463; G_ENABLED_IDPS=google; _gac_UA-29776811-12=1.1665471561.Cj0KCQjwhY-aBhCUARIsALNIC044jf7vH3ak5X5f1fIcAPa0bOcqyAEuU2T87_X63FklWPt5T-FESZQaAvofEALw_wcB; tvl=qgdHX7GvehrD9XH5a3S4PdE8AYpuF3hYPaT5bxhY7Zb1rMY5TK425kIv5ETJRXLCHeSRMonmaMv0od7JDeBrc6DBkhayi5gQ9Dpvi2lz6w2oLTfDdPL1xT79VU0AZGQMM9o9QzWqp1ZS/a118SM5D+1Jxf+oecGs1Um1EPNjo8VURN3ThgD8Mf5lG13Ci4j5hI7fqyCyEmdQUhtDwGw8cVjAoQJMjZTOsRzZHor+vWeay3MPPOB9sZ/UZ/Krc5UECFnqYYpMSCRL8yPQAXFPMQ==; _gid=GA1.2.379556567.1666197141; tv-repeat-visit=true; selectedCurrency=EUR; selectedLocale=en_EN; _ga_RSRSMMBH0X=GS1.1.1666233648.18.1.1666233694.14.0.0; tvs=qgdHX7GvehrD9XH5a3S4PXWKx93/3Xi103f/kPpnhg1IQez7AjqOPow88qqCMiL7CqvJjpn5Z2svD8QZzAmUN8I7bk86ki9gpnRvB7n1SMg3zgM8GEQUzXoABab35F/lt0q47TyT+ecCF4TDzdkuTYR+PTB5vVtlAZSjbXhlEL9MVeoaxEQheXCzmm0cEa6sfX20ZzuJzpCR3ljpziEyvAE6sRwDYjvvt4ACkV8PmtfgQe/dpDKe1splNHwbCZYHHKMAHEOIS2CR6oHc11VkcMOrX6BQKO3p75zJ993N5ctP5lUUeZvqwg8Za/5sXO3LZP+wb33mpHnKyxjbfd5ZlTN3hMzImFcyzvIC2wefxfo=; amp_1a5adb=W28TgfofcjM3MKtAsufi5f.MjU0NjAwMDIw..1gfpk7gvl.1gfpk8vhl.7o.0.7o; cto_bundle=XKyIrl9XcGVTRjV3eWttJTJCbWQzRzQlMkZobSUyRklvTWFzTUJ2cWVXaTJrVGE2ZG5SRzdHWmpLR05BOTEzRGV4bGxpTnZLU0x4TWxaS3I2cUNpUHVCdWkySFJnUGRuSSUyQmdNajREbmhnWlZIZ05XQ2t1ZnEwb0pNYnVkJTJGUiUyQmtvMjlIJTJCWGM5RVRGalhKZlRjbWR5czlxSW9WeTZoY1BpUSUzRCUzRA; _ga=GA1.2.677393933.1664942594',
            'origin': 'https://www.traveloka.com',
            'referer': referer,
            'sec-ch-ua': '"Chromium";v="106", "Google Chrome";v="106", "Not;A=Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',
            'x-domain': 'accomContent',
            'x-route-prefix': 'en-en',
        }

    skip = 0
    top = 50
    count = 0

    while True:
            json_data = {
                'fields': [],
                'data': {
                    'filterSortSpec': {
                        'travelTheme': None,
                        'travelType': None,
                        'sortType': 'LANGUAGE',
                        'tagIds': [],
                    },
                    'ascending': True,
                    'reviewLanguage': 'ENGLISH',
                    'hotelId': hotelId,
                    'skip': skip,
                    'top': top,
                },
                'clientInterface': 'desktop',
            }
            try:
                response = requests.post('https://www.traveloka.com/api/v2/hotel/getHotelReviews', cookies=cookies, headers=headers, json=json_data)
                json_travel = response.json()
                collection = []
                review_list = json_travel['data']['reviewList']
                print(len(review_list))
                count += len(review_list)
                if (len(review_list) == 0):
                    break
                for review in review_list:
                    try:
                        reviewTen = review['reviewerName']
                        reviewContent = review['reviewText']
                        score = review['overallScore']
                        referer = review['sharedUrl']

                        danhgia = {
                            'reviewTen' : reviewTen,
                            'reviewContent' : reviewContent,
                            'score' : score,
                            'referer' : referer
                        }

                        collection.append(danhgia)
                        df = pd.DataFrame(collection)
                    except Exception as e:
                          print('')
                skip += top
                print(df)


            except:
                    pass

dfHotel = pd.read_csv("Data_review.csv")
dfHotel.info()

dfHotel

"""Choose specific column for Data"""

df_review = dfHotel[['reviewContent']]
df_review

"""### EDA

Drop unnamed column
"""

from google.colab import drive
drive.mount('/content/drive')

dfHotel = pd.read_csv("Data_review.csv")
dfHotel.head(5)
# To display the top 5 rows

dfHotel.tail(5)
# To display the botton 5 rows

"""Checking the types of data"""

dfHotel.dtypes

"""Dropping irrelevant columns"""

dfHotel = dfHotel.drop(['referer'], axis=1)

"""Renaming the columns"""

dfHotel = dfHotel.rename(columns={"reviewTen": "Name", "reviewContent": "Review", "score": "Score" })
dfHotel.head(5)

"""Dropping the duplicate rows"""

dfHotel.shape

duplicate_rows_df = dfHotel[dfHotel.duplicated()]
print("number of duplicate rows: ", duplicate_rows_df.shape)

"""remove the duplicate data"""

dfHotel.count()
# Used to count the number of rows

dfHotel = dfHotel.drop_duplicates()
dfHotel.head(5)

dfHotel.count()

"""Dropping the missing or null values."""

print(dfHotel.isnull().sum())

dfHotel = dfHotel.dropna()    # Dropping the missing values.
dfHotel.count()

print(dfHotel.isnull().sum())

"""Detecting Outliers"""

sns.boxplot(x=dfHotel['Score'])

dfHotel.info()

"""Customize df"""

df_db = dfHotel['Review']

"""Word display"""

#1. Total No of Char
df_db['num_char'] = df_db.apply(len)
#2. Total No of Words
df_db['num_words'] = df_db.apply(lambda x: len(str(x).split()))
#3. avg word length
df_db['avg_words_length'] = df_db['num_char']/df_db['num_words']

df_db

df_db.describe()

sns.set(rc={"figure.figsize":(20, 10)})
sns.boxplot(data=df_db[["num_char", "num_words","avg_words_length"]], orient="v", width=0.5)

# Construct the scatter plot
p = sns.scatterplot(data=df_db, x="num_char", y="num_words",palette="deep")

"""#### Outliner"""

# IQR
Q1 = np.percentile(df_db['num_char'], 25, interpolation = 'midpoint')
Q3 = np.percentile(df_db['num_char'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
print("Inter Quartile Range:",IQR)

# IQR
Q1 = np.percentile(df_db['num_words'], 25, interpolation = 'midpoint')
Q3 = np.percentile(df_db['num_words'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
print("Inter Quartile Range:",IQR)
print("Upper fence:", Q3+1.5*IQR)

# IQR
Q1 = np.percentile(df_db['avg_words_length'], 25, interpolation = 'midpoint')
Q3 = np.percentile(df_db['avg_words_length'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
print("Inter Quartile Range:",IQR)
print("Upper fence:", Q3+1.5*IQR)
print("Lower fence:", Q1-1.5*IQR)

"""#### Display"""

df_db.loc[df_db.num_char > 304.0]

df_db.loc[df_db.num_words > 49.5]

df_db.loc[df_db.avg_words_length > 10.305934195064632]

df_db.loc[df_db.avg_words_length <= 2.7764394829612207]

df_db = df_db.loc[df_db.avg_words_length <= 10.305934195064632]

"""## Pre-Processing

### Cleaning

Check and Remove all duplicated reviews
"""

df_review.duplicated().sum()

df_review.drop_duplicates(inplace=True)

df = df_review
df

"""#### Creating a new function to Delete special symbol/ word"""

#Remove punctuation
df['reviewContent'] = df['reviewContent'].apply(lambda x: re.sub('[,\.!?]', '', str(x)))

df['reviewContent'] = df['reviewContent'].apply(lambda x : str(x.lower()))

"""Remove stopword"""

stop = stopwords.words('english')

"""### Customize Cleaning

####Custome library
"""

apostrophe_dict = {
"ain't": "am not / are not",
"aren't": "are not / am not",
"can't": "cannot",
"can't've": "cannot have",
"cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he had / he would",
"he'd've": "he would have",
"he'll": "he shall / he will",
"he'll've": "he shall have / he will have",
"he's": "he has / he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how has / how is",
"i'd": "I had / I would",
"i'd've": "I would have",
"i'll": "I shall / I will",
"i'll've": "I shall have / I will have",
"i'm": "I am",
"i've": "I have",
"isn't": "is not",
"it'd": "it had / it would",
"it'd've": "it would have",
"it'll": "it shall / it will",
"it'll've": "it shall have / it will have",
"it's": "it has / it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she had / she would",
"she'd've": "she would have",
"she'll": "she shall / she will",
"she'll've": "she shall have / she will have",
"she's": "she has / she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so as / so is",
"that'd": "that would / that had",
"that'd've": "that would have",
"that's": "that has / that is",
"there'd": "there had / there would",
"there'd've": "there would have",
"there's": "there has / there is",
"they'd": "they had / they would",
"they'd've": "they would have",
"they'll": "they shall / they will",
"they'll've": "they shall have / they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"we'd": "we had / we would",
"we'd've": "we would have",
"we'll": "we will",
"we'll've": "we will have",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what shall / what will",
"what'll've": "what shall have / what will have",
"what're": "what are",
"what's": "what has / what is",
"what've": "what have",
"when's": "when has / when is",
"when've": "when have",
"where'd": "where did",
"where's": "where has / where is",
"where've": "where have",
"who'll": "who shall / who will",
"who'll've": "who shall have / who will have",
"who's": "who has / who is",
"who've": "who have",
"why's": "why has / why is",
"why've": "why have",
"will've": "will have",
"won't": "will not",
"won't've": "will not have",
"would've": "would have",
"wouldn't": "would not",
"wouldn't've": "would not have",
"y'all": "you all",
"y'all'd": "you all would",
"y'all'd've": "you all would have",
"y'all're": "you all are",
"y'all've": "you all have",
"you'd": "you had / you would",
"you'd've": "you would have",
"you'll": "you shall / you will",
"you'll've": "you shall have / you will have",
"you're": "you are",
"you've": "you have",
"apple‚Äôs":"apple is",
"dxomark's":"dxomark is"
}
apostrophe_dict

short_word_dict = {
"121": "one to one",
"1/2":"half",
"adn": "any day now",
"afaik": "as far as I know",
"afk": "away from keyboard",
"aight": "alright",
"alol": "actually laughing out loud",
"b4": "before",
"b4n": "bye for now",
"bak": "back at the keyboard",
"bta": "but then again",
"btw": "by the way",
"cid": "crying in disgrace",
"cnp": "continued in my next post",
"cyo": "see you online",
"dbau": "doing business as usual",
"fud": "fear, uncertainty, and doubt",
"fwiw": "for what it's worth",
"fyi": "for your information",
"g": "grin",
"g2g": "got to go",
"ga": "go ahead",
"gal": "get a life",
"gf": "girlfriend",
"gfn": "gone for now",
"gmbo": "giggling my butt off",
"gmta": "great minds think alike",
"h8": "hate",
"hagn": "have a good night",
"hdop": "help delete online predators",
"iac": "in any case",
"ianal": "I am not a lawyer",
"ic": "I see",
"idk": "I don't know",
"imao": "in my arrogant opinion",
"imnsho": "in my not so humble opinion",
"imo": "in my opinion",
"iow": "in other words",
"ipn": "I‚Äôm posting naked",
"irl": "in real life",
"jk": "just kidding",
"l8r": "later",
"ldr": "long distance relationship",
"llta": "lots and lots of thunderous applause",
"lmao": "laugh my ass off",
"lol": "laugh out loud",
"ltr": "longterm relationship",
"luv": "love",
"m/f": "male or female",
"m8": "mate",
"milf": "mother I would like to fuck",
"oll": "online love",
"omg": "oh my god",
"otoh": "on the other hand",
"pir": "parent in room",
"ppl": "people",
"r": "are",
"rpg": "role playing games",
"ru": "are you",
"shid": "slaps head in disgust",
"somy": "sick of me yet",
"sot": "short of time",
"thanx": "thanks",
"thx": "thanks",
"u": "you",
"ur": "you are",
"uw": "you‚Äôre welcome",
"wb": "welcome back",
"wfm": "works for me",
"wibni": "wouldn't it be nice if",
"wtf": "what the fuck",
"wtg": "way to go",
"wtgp": "want to go private",
"ym": "young man",
"gr8": "great",
"etc":"et cetera"
}

icon_lib = {
    "ü§£":"laugh",
    "üíî":"hate",
    "üòå":"pleased",
    "üòÇ":"funny",
    "ü§î":"think",
    "‚úÖ":"yes",
    "üóø":"speechless",
    "üôèüèø":"please",
    "ü§©":"delighted",
    "üî•":"hot",
    "üò≠":"cry",
    "‚ö°Ô∏è":"fast",
    "‚ù§Ô∏è":"love",
    "üòä":"happy",
    "ü•∞":"thanks",
    "üò¢":"unfortunate",
    "‚òπÔ∏è":"sad",
    "‚ùå":"no",
    "üëçüèª":"okay",
    "üí•":"hot",
    "üí∏":"expensive",
    "üíØ":"nice",
    "üòí":"disappointed",
    "ü§Ø":"surprised",
    "ü§≤üèø":"want",
    "üòÇ":"hilarious",
    "ü•≥":"celebrate",
    "üôÇ":"normal"
}

"""#### Proceed"""

def lookup_dict(text, dictionary):
    for word in text.split():
        if word.lower() in dictionary:
            if word.lower() in text.split():
                text = text.replace(word, dictionary[word.lower()])
    return text

df['reviewContent'] = df['reviewContent'].apply(lambda x: lookup_dict(x,apostrophe_dict))
df

df['reviewContent'] = df['reviewContent'].apply(lambda x: lookup_dict(x,short_word_dict))
df

df['reviewContent'] = df['reviewContent'].apply(lambda x: lookup_dict(x,icon_lib))
df

"""### Stemming & Lemmatizer"""

#POS tagging to exclude word like has/was...
df['lemmatizer'] = df['reviewContent'].apply(lambda x : ' '.join(lemmatizer.lemmatize(word, pos="v") for word in x.split()))

df['comment_word_token'] = df['lemmatizer'].apply(lambda x: ' '.join(word for word in word_tokenize(x) if word not in (stop)))

print(df['comment_word_token'].to_numpy())

df

"""#### Divide into train and test database"""

df_new = df[['reviewContent','comment_word_token']]

df_train, df_test = train_test_split(df_new, test_size=0.2, random_state=0)

df = df_train.reset_index(drop=True)

"""### Rating / Score

#### Polarity Rating
"""

polarity_score = []

for text in df['reviewContent']:
  blob = TextBlob(text)
  for sentence in blob.sentences:
    if sentence.sentiment.polarity > 0.0:
      polarity_score.append('positive')
    elif sentence.sentiment.polarity < 0.0:
      polarity_score.append('negative')
    else:
      polarity_score.append('neutral')

df['score'] = pd.DataFrame(polarity_score)

df

df = pd.read_csv("Update_Data.csv", encoding='latin-1')

df

ax = sns.countplot(df['score'])
plt.sca(ax)
plt.xticks(rotation=90);

plt.savefig("Score.png")

df['rating'] = df['score']
df.rating = df.rating.str.replace('positive', '1')
df.rating = df.rating.str.replace('neutral', '0')
df.rating = df.rating.str.replace('negative', '-1')

temp = df.groupby('score').count()['reviewContent'].reset_index().sort_values(by='reviewContent',ascending=False)
temp.style.background_gradient(cmap='Purples')

from plotly import graph_objs as go
fig = go.Figure(go.Funnelarea(
    text =temp.score,
    values = temp.reviewContent,
    title = {"position": "top center", "text": "Funnel-Chart of ."}
    ))
fig.show()

"""#### VADER rating"""

# function to calculate vader sentiment
def vadersentimentanalysis(review):
    vs = analyzer.polarity_scores(review)
    return vs['compound']
    df['Vader data'] = df['text'].apply(vadersentimentanalysis)

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
def vadar_sentiment(text):
    return analyzer.polarity_scores(text)['compound']

# create new column for vadar compound sentiment score
df['vadar compound'] = df['reviewContent'].apply(vadar_sentiment)

def categorise_sentiment(sentiment, neg_threshold=-0.05, pos_threshold=0.05):
    if sentiment < neg_threshold:
        label = 'negative'
    elif sentiment > pos_threshold:
        label = 'positive'
    else:
        label = 'neutral'
    return label

# new col with vadar sentiment label based on vadar compound score
df['vadar score'] = df['vadar compound'].apply(categorise_sentiment)

df

df['score']

vader_counts = df['vadar score'].value_counts()
tb_counts = df['score'].value_counts()

plt.figure(figsize=(15,7))
plt.subplot(1,2,1)
plt.title("TextBlob results")
plt.pie(tb_counts.values, labels = tb_counts.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)
plt.subplot(1,2,2)
plt.title("VADER results")
plt.pie(vader_counts.values, labels = vader_counts.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)

"""### WordCloud"""

all_words = ' '.join([text for text in df['comment_word_token']])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common words around hotels in Binh Thuan release")
plt.show()

negative_words =' '.join([text for text in df['comment_word_token'][df['score'] == 'negative']])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common negative words around hotels in Binh Thuan release")
plt.show()

positive_words =' '.join([text for text in df['comment_word_token'][df['score'] == 'positive']])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common positive words around hotels in Binh Thuan release")
plt.show()

neutral_words =' '.join([text for text in df['comment_word_token'][df['score'] == 'neutral']])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neutral_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Common neutral words around hotels in Binh Thuan release")
plt.show()

"""## Machine learning

### Bag of words
"""

# Chuy·ªÉn vƒÉn b·∫£n th√†nh vect∆° tr√™n c∆° s·ªü t·∫ßn su·∫•t c·ªßa m·ªói t·ª´ xu·∫•t hi·ªán trong to√†n b·ªô vƒÉn b·∫£n.
# X√≥a c√°c stopwords xu·∫•t hi·ªán th∆∞·ªùng xuy√™n trong h∆°n 50% c·ªßa t√†i li·ªáu
# X√≥a c√°c stopwords xu·∫•t hi·ªán kh√¥ng th∆∞·ªùng xuy√™n trong √≠t h∆°n 2 t√†i li·ªáu
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english',ngram_range=(1,2))
bow_vectorizer

# duy·ªát qua comment_word_token v√† tr·∫£ v·ªÅ ma tr·∫≠n t·ª´ ng·ªØ
bow = bow_vectorizer.fit_transform(df['comment_word_token'])
bow

# T√≠nh t·∫•t c·∫£ t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa n k√≠ t·ª± (n-grams) li√™n ti·∫øp ƒë∆∞·ª£c t√¨m th·∫•y trong t·∫•t c·∫£ d·ªØ li·ªáu
list(islice(bow_vectorizer.vocabulary_.items(), 20))

"""### TF - IDF"""

vectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,2))

train_data_features = vectorizer.fit_transform(df['comment_word_token'])
print(train_data_features.shape)

train,test = train_test_split(df, test_size=0.2, shuffle=True) # chia t·ªáp d·ªØ li·ªáu df ra l√†m 2 ph·∫ßn, ki·ªÉm th·ª≠ 20% v√† hu·∫•n luy·ªán 80% v√† c√≥ chia ng·∫´u nhi√™n
vectorizer = TfidfVectorizer() # t·∫°o ma tr·∫≠n
# ƒë∆∞a nh·ªØng t·ª´ c√≥ trong c·ªôt comment_word_token v√†o ma tr·∫≠n
vectorizer.fit_transform(train.comment_word_token)
vectorizer.fit_transform(test.comment_word_token)
X_train = vectorizer.transform(train.comment_word_token) # t·∫°o ma tr·∫≠n ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh
Y_train = train.score
X_test = vectorizer.transform(test.comment_word_token) # t·∫°o ma tr·∫≠n ƒë·ªÉ ki·ªÉm th·ª≠ m√¥ h√¨nh
Y_test = test.score

temp = pd.DataFrame(X_train.todense(),columns = vectorizer.get_feature_names_out())
temp

"""### Random Forest"""

df

"""###SVM"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
classifier_linear = SVC(kernel='linear')
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
classifier_linear = SVC(kernel='sigmoid')
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

"""

```
# ƒê·ªãnh d·∫°ng c·ªßa ƒëo·∫°n n√†y l√† m√£
```

### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

classifier_linear = LogisticRegression()
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(classification_report(Y_test, pred))

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

classifier_linear = DecisionTreeClassifier()
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

"""

```
# This is formatted as code
```

## Applying ML to test database"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
classifier_linear = SVC(kernel='rbf')
classifier_linear.fit(X_train, Y_train)
pred = classifier_linear.predict(X_test)
print(accuracy_score(Y_test, pred))
print(classification_report(Y_test, pred))

test['pred_score'] = pred
test